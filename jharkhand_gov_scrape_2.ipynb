{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): bs4 in /anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): beautifulsoup4 in /anaconda/lib/python2.7/site-packages (from bs4)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import datetime\n",
    "import urllib\n",
    "#!conda install -y bs4\n",
    "!pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# the below gives pandas the ability to grab a html table\n",
    "#!conda install -y html5lib\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def get_dist_ids():\n",
    "#   \"\"\"Returns the district IDs from the site.\"\"\"\n",
    "#   dist_url = 'http://aahar.jharkhand.gov.in/transactions/transactionView'\n",
    "#   rids = requests.get(dist_url)\n",
    "#  content = rids.text\n",
    "#   soup = BeautifulSoup(content, \"html.parser\")\n",
    "#  dist_ids = [(option['value'],option.text) for option in soup.find_all('option', value=True)[1:-2]]\n",
    "#   return dist_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(get_dist_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_block_ids(d_id):\n",
    "    \"\"\"Returns a list of block ids.\"\"\"\n",
    "    block_muni = 'http://aahar.jharkhand.gov.in/dealer_user_logs/getBlockMunicipality'\n",
    "    dist_post = {'data[DealerUserLog][district_id]':d_id}\n",
    "    r = requests.post(block_muni, data=dist_post)\n",
    "    content = r.text\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    block_ids = [(option['value'],option.text) for option in soup.find_all('option', value=True)[1:]]\n",
    "    #print(block_ids)\n",
    "    return block_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Want data from 15-8-2016 to 30-11-2016\n",
    "def get_dates():\n",
    "    numdays = 109\n",
    "    base = datetime.date.today()\n",
    "    date_list = [(base - datetime.timedelta(days=x)).strftime(\"%d-%m-%Y\") for x in range(0, numdays)]\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dealer_links(block_post):\n",
    "    r = requests.post(\"http://aahar.jharkhand.gov.in/dealer_user_logs/hhdLoginDetail\", data=block_post)\n",
    "    #data = r.text\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    main_table = soup.find('table', id='maintable')\n",
    "    tmp_link_list = []\n",
    "    for link in main_table.find_all('a', href=True):\n",
    "        tmp_link_list.append(link['href'])\n",
    "    return tmp_link_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_transactions(base_url, dealer_link_list, d_id,block_key, block_value, dt):\n",
    "    \"\"\"Returns transactions.\"\"\"\n",
    "    \n",
    "    for link in dealer_link_list:\n",
    "        if urllib.urlopen(base_url+link).read().decode('utf-8').find(\"SORRY!!!! No record found....\") == -1:\n",
    "            tmp_data = pd.read_html(base_url+link)[0][1:]\n",
    "            tmp_data.columns = tmp_data.iloc[0]\n",
    "            tmp_data = tmp_data[1:] #take the data less the header row  \n",
    "            tmp_data['district_id'] = pd.Series(d_id, index=tmp_data.index)\n",
    "            tmp_data['date'] = pd.Series(dt, index=tmp_data.index)\n",
    "            tmp_data['block_name'] = pd.Series(block_value, index=tmp_data.index)\n",
    "            tmp_data['block_id'] = pd.Series(block_key, index=tmp_data.index)\n",
    "        \n",
    "            if os.path.isfile('./test.csv') == False:\n",
    "                tmp_data.to_csv('test.csv', encoding='utf-8')\n",
    "            else:\n",
    "                with open('test.csv', 'a') as f:\n",
    "                    tmp_data.to_csv(f, header=False, encoding='utf-8')\n",
    "        \n",
    "        #return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b786d8d9b59c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m#print(block_post)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#dealer_link_list.append(get_dealer_links(block_post))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mget_transactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dealer_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_post\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#need a function that makes requests and processes responses, then calls function to build csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-49b64a3affac>\u001b[0m in \u001b[0;36mget_transactions\u001b[0;34m(base_url, dealer_link_list, d_id, block_key, block_value, dt)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdealer_link_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SORRY!!!! No record found....\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtmp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtmp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtmp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#take the data less the header row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/html.pyc\u001b[0m in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, tupleize_cols, thousands, encoding)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[0m_validate_header_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m     return _parse(flavor, io, match, header, index_col, skiprows,\n\u001b[0;32m--> 874\u001b[0;31m                   parse_dates, tupleize_cols, thousands, attrs, encoding)\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/html.pyc\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, header, index_col, skiprows, parse_dates, tupleize_cols, thousands, attrs, encoding)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mretained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaught\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/html.pyc\u001b[0m in \u001b[0;36mparse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/html.pyc\u001b[0m in \u001b[0;36m_build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         return BeautifulSoup(self._setup_build_doc(), features='html5lib',\n\u001b[0m\u001b[1;32m    451\u001b[0m                              from_encoding=self.encoding)\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/html.pyc\u001b[0m in \u001b[0;36m_setup_build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No text parsed from document: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/html.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#List for dealer links\n",
    "#dealer_link_list = []\n",
    "base_url = \"http://aahar.jharkhand.gov.in\"\n",
    "dist_ids = list(range(1,25))\n",
    "\n",
    "for d_id in dist_ids:\n",
    "    b_ids = get_block_ids(d_id)\n",
    "    for key, value in b_ids:\n",
    "        date_list = get_dates()[1:3]\n",
    "        for dt in date_list: \n",
    "            #print(dt)\n",
    "            block_post = {'data[DealerUserLog][district_id]': d_id,\n",
    "                      'data[DealerUserLog][block_city_id]': key,\n",
    "                      'data[DealerUserLog][stockdatefrom]': dt }\n",
    "            #print(block_post)\n",
    "            #dealer_link_list.append(get_dealer_links(block_post))\n",
    "            get_transactions(base_url, get_dealer_links(block_post), d_id,key, value, dt)\n",
    "            \n",
    "    #need a function that makes requests and processes responses, then calls function to build csv \n",
    "            \n",
    "#print(dealer_links_list[1:10])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-8-210c8040bd87>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-210c8040bd87>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    for row in data:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def append_csv(data):\n",
    "#    with open('foo.csv', 'a') as f:\n",
    "#             (data).to_csv(f, header=False)\n",
    "            \n",
    "            \n",
    "def write_csv(data):\n",
    "    # we need to test to see if file exists first\n",
    "    # we need to create a file if it doesn't exist \n",
    "    # come up with naming schema - best to prob. use dates parameters used in query\n",
    "    # we need to add the dealers (the dealer name is in hindi in the table), district, block and other identifiers\n",
    "    # below is the basic writing out operation for a row list, we'll need to change it some to make it\n",
    "    # work with the above and then appending new rows\n",
    "    csvFile = open(\"files/transactions.csv\", 'wt')\n",
    "    writer = csv.writer(csvFile)\n",
    "    #try:\n",
    "        for row in data:\n",
    "           csvRow = []\n",
    "            for cell in row.findAll(['td', 'th']):\n",
    "                csvRow.append(cell.get_text())\n",
    "            writer.writerow(csvRow)\n",
    "    #finally:\n",
    "        csvFile.close()\n",
    "    pass #tell python not to run function, remove when you want to run it\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = \"/transactions/hhdLoginTransaction/NGYwN2U0Y2YtZThjNC00OWUxLTg1ZGQtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\"\n",
    "#print(link)\n",
    "base_url = \"http://aahar.jharkhand.gov.in\"\n",
    "if urllib.urlopen(base_url+link).read().decode('utf-8').find(\"SORRY!!!! No record found....\") != -1:\n",
    "    print('TRUE')\n",
    "#print(base_url+link)\n",
    "web = urllib.urlopen(base_url+link)\n",
    "data = web.read()\n",
    "print(data.decode().find(\"SORRY!!!! No record found....\"))\n",
    "#soup = BeautifulSoup(data, \"html.parser\")\n",
    "#main_table = soup.find('table', id='maintable')\n",
    "#print(main_table)\n",
    "#print(main_table.th.get_text() for th in table.find(\"tr\").find_all(\"th\"))\n",
    "\n",
    "link1 = \"/transactions/hhdLoginTransaction/NGY2MmQ3ODYtYjFiOC00NDYwLTkzYzAtMGRmODBhODZiOTA0/MjQ=/MjAxNi0xMi0wOA==\"\n",
    "urllib.urlopen(base_url+link1).read().decode('utf-8').find(\"SORRY!!!! No record found....\")\n",
    "print(base_url+link1)\n",
    "d_id = 1\n",
    "\n",
    "data = pd.read_html(base_url+link1)[0][1:]\n",
    "data.columns = data.iloc[0]\n",
    "data = data[1:] #take the data less the header row\n",
    "\n",
    "data['district_id'] = pd.Series(d_id, index=data.index)\n",
    "data\n",
    "#data[0][1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "link = \"/transactions/hhdLoginTransaction/NGYwN2U0Y2YtZThjNC00OWUxLTg1ZGQtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\"\n",
    "#print(link)\n",
    "base_url = \"http://aahar.jharkhand.gov.in\"\n",
    "#print(base_url+link)\n",
    "#web = urllib.urlopen(base_url+link)\n",
    "#data = web.read()\n",
    "#print(data.decode().find(\"SORRY!!!! No record found....\"))\n",
    "#soup = BeautifulSoup(data, \"html.parser\")\n",
    "#main_table = soup.find('table', id='maintable')\n",
    "#print(main_table)\n",
    "#print(main_table.th.get_text() for th in table.find(\"tr\").find_all(\"th\"))\n",
    "\n",
    "link1 = \"/transactions/hhdLoginTransaction/NGY2MmQ3ODYtYjFiOC00NDYwLTkzYzAtMGRmODBhODZiOTA0/MjQ=/MjAxNi0xMi0wOA==\"\n",
    "print(base_url+link1)\n",
    "data = pd.read_html(base_url+link1)\n",
    "\n",
    "data[0]\n",
    "\n",
    "#web = urllib.urlopen(base_url+link1)\n",
    "#data = web.read()\n",
    "#soup = BeautifulSoup(data, \"html.parser\")\n",
    "#soup = BeautifulSoup(data, \"lxml\")\n",
    "\n",
    "#soup.prettify()\n",
    "#main_table = soup.find('table', id='maintable')\n",
    "#print(main_table.find_all('tr')[7:])\n",
    "#print(main_table)\n",
    "\n",
    "data = {\n",
    "    'obs_number' : [],\n",
    "    'dealer_name' : [],\n",
    "    'day': [],\n",
    "    'date_time' : [],\n",
    "    'ration_card_number' : [],\n",
    "    'present_cardtype' : [],\n",
    "    'old_cardtype' : [],\n",
    "    'hh_uid' : [],\n",
    "    'member_name' : [],\n",
    "    'rice_kg' : [],\n",
    "    'wheat_kg' : [],\n",
    "    'kerosene_lit' : [],\n",
    "    'salt_kg' : []\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "#rows = [tr.findAll('td') for tr in main_table.findAll('tr')[7:]]\n",
    "rows = main_table.find_all('tr')[2:]\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "\n",
    "\n",
    "#cols = row.find_all('td')\n",
    "    data['obs_number'].append( cols[0].get_text() )\n",
    "    data['dealer_name'].append( cols[1].get_text() )\n",
    "    data['date_time'].append( cols[2].get_text() )\n",
    "    data['ration_card_number'].append( cols[3].get_text() )\n",
    "    data['present_cardtype'].append( cols[4].get_text() )\n",
    "    data['old_cardtype'].append( cols[5].get_text() )\n",
    "    data['hh_uid'].append( cols[6].get_text() )\n",
    "    data['member_name'].append( cols[7].get_text() )\n",
    "    data['rice_kg'].append( cols[8].get_text() )\n",
    "    data['wheat_kg'].append( cols[9].get_text() )\n",
    "    data['kerosene_lit'].append( cols[10].get_text() )\n",
    "    data['salt_kg'].append( cols[11].get_text() )\n",
    "\n",
    "#print(data)\n",
    "#import numpy\n",
    "for key,values in data.iteritems():\n",
    "    print(key)\n",
    "    for value in values:\n",
    "        print(value)\n",
    "    \n",
    "\n",
    "#data = pd.DataFrame( data )\n",
    "#dogData.to_csv(\"AKC_Dog_Registrations.csv\")\n",
    "\n",
    "\n",
    "#print(rows)\n",
    "#th are table headers variable names\n",
    "#tr are new rows\n",
    "#td are the data cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tmp_link_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/transactions/hhdLoginTransaction/NGYwN2UzMTYtNmU1MC00M2RjLTg0Y2MtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2U0Y2YtZThjNC00OWUxLTg1ZGQtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2U2NjUtMzU4OC00MDEyLTgzMGItMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2ViNDItZmNmOC00ZWE5LWEyZjgtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2YwOTQtN2EwNC00YTQwLTk2YjEtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2Y0YjYtNjhlYy00ZjEwLTk2ZWQtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2Y3YmQtNGNhMC00MWIxLWEwMWUtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2ZhNTAtZWMzMC00MDI0LTk5YmUtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwN2ZiOTgtZjg2MC00Mzc5LWFlY2MtMGVkODBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwYWJlNzQtMzA1MC00OWNiLWI5NTgtMTNiYzBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwYWJmNDktODdkYy00NmM1LTk2Y2QtMTNiYzBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwYWMwYjMtY2ViYy00MDFhLTljYTQtMTNiYzBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NGYwYWMxOWMtOGViOC00NTE2LTkwYjktMTNiYzBhODY3OTFi/MQ==/MjAxNi0xMC0xNw==\n",
      "/transactions/hhdLoginTransaction/NTAwNTE1NmYtNjgzMC00Yjk0LTllM2EtMGZhMDBhODY3OTA2/MQ==/MjAxNi0xMC0xNw==\n"
     ]
    }
   ],
   "source": [
    "#print(dealer_links_list[1:10])\n",
    "\n",
    "for links in tmp_link_list:\n",
    "    print(links['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transactions1(base_url, dealer_link_list, d_id, block_value, dt):\n",
    "    \"\"\"Returns transactions.\"\"\"\n",
    "    for link in dealer_link_list:\n",
    "        #data = pd.read_html(base_url+link1)\n",
    "        data = urllib.urlopen(base_url+link).read()\n",
    "        if data.decode().find(\"SORRY!!!! No record found....\") != -1:\n",
    "            continue\n",
    "        soup = BeautifulSoup(data, \"lxml\")\n",
    "        main_table = soup.find('table', id='maintable')\n",
    "        rows = main_table.find_all('tr')[2:]\n",
    "        \n",
    "        data = {\n",
    "            'district': []\n",
    "            'block': []\n",
    "            'date': []\n",
    "            'obs_number' : [],\n",
    "            'dealer_name' : [],\n",
    "            'date_time' : [],\n",
    "            'ration_card_number' : [],\n",
    "            'present_cardtype' : [],\n",
    "            'old_cardtype' : [],\n",
    "            'hh_uid' : [],\n",
    "            'member_name' : [],\n",
    "            'rice_kg' : [],\n",
    "            'wheat_kg' : [],\n",
    "            'kerosene_lit' : [],\n",
    "            'salt_kg' : []\n",
    "        }\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            data['district'].append( d_id )\n",
    "            data['block'].append( block_value )\n",
    "            data['date'].append( dt )\n",
    "            data['obs_number'].append( cols[0].get_text() )\n",
    "            data['dealer_name'].append( cols[1].get_text() )\n",
    "            data['date_time'].append( cols[2].get_text() )\n",
    "            data['ration_card_number'].append( cols[3].get_text() )\n",
    "            data['present_cardtype'].append( cols[4].get_text() )\n",
    "            data['old_cardtype'].append( cols[5].get_text() )\n",
    "            data['hh_uid'].append( cols[6].get_text() )\n",
    "            data['member_name'].append( cols[7].get_text() )\n",
    "            data['rice_kg'].append( cols[8].get_text() )\n",
    "            data['wheat_kg'].append( cols[9].get_text() )\n",
    "            data['kerosene_lit'].append( cols[10].get_text() )\n",
    "            data['salt_kg'].append( cols[11].get_text() )\n",
    "\n",
    "    return data\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
